<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	
	<head>
		<title>NIHARIKA'S PORTFOLIO</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
		<style>
			/* Remove list styling */
			nav ul {
			  list-style: none;
			  padding: 0;
			}
		
			/* Style anchor links */
			nav a {
			  text-decoration: none;
			  color: #333; /* Change the color as needed */
			}
		
			/* Add bullet points and center them */
			ul {
			  list-style-position: inside;
			  text-align: center;
			}
			#main h2 {
      font-size: 1.5em; /* Adjust the value as needed */
    }

    #main ul {
      font-size: 1.2em; /* Adjust the value as needed */
    }

    /* Increase font size for the table of contents items */
    #main ul li {
      font-size: 1.2em; /* Adjust the value as needed */
    }
		  </style>
	</head>

	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper" class="fade-in">

				<!-- Intro -->
					<div id="intro">
						<h1>Niharika Kolliboyana<br />
						</h1>
						<p>Data Analyst skilled in Analytical Tools with a Dash of Machine Learning</p>
						<ul class="actions">
							<li><a href="#header" class="button icon solid solo fa-arrow-down scrolly">Continue</a></li>
						</ul>
					</div>

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo">Let's Jump!</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li class="active"><a href="index.html">MY PORTFOLIO</a></li>
			
						</ul>
						<ul class="icons">
							<li><a href="https://www.linkedin.com/in/niharikabk/" class="icon brands fa-linkedin"><span class="label">linkedin</span></a></li>
				
							<li><a href="https://github.com/Niharika1307" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
							<li><a href="https://newopinions8.wordpress.com/" class="icon brands fa-wordpress"><span class="label">Blog</span></a></li>
						</ul>
					</nav>
					
				<div id="main">
				
						<!-- Featured Post -->
							<article class="post featured">
								<header class="major">
						<h2>Table of Contents</h2>
					<ul>
					      <li><a href="#section1">About Me</a></li>
						  <li><a href="#section2">Skills</a></li>
						  <li><a href="#section3">Experience</a></li>
						  <li><a href="#section4">Projects</a></li>
						  <li><a href="#section5">Research</a></li>
						  <li><a href="#section6">Case Study</a></li>
						  <li><a href="#section7">Education</a></li>
						  <li><a href="#section8">Certifications</a></li>
						  <!-- Add more sections as needed -->
					</ul>
					<hr>
					<div id="main">

						<!-- Featured Post -->
						
							<article class="post featured">
								<header class="major">
				
							        <h2>ABOUT  ME<br />
									</a></h2>
									<p>
										From the moment I first encountered Jim Bergeson's wisdom, "Data will talk to you if you're willing to listen," I knew that this would guide my journey. It all began during my bachelor's degree in Computer Science, where I was captivated by the power of programming languages and algorithms. This fascination naturally led me to the world of machine learning and data analysis, where I discovered the potential to uncover hidden patterns and insights.

My passion for deep learning grew as I delved into research, culminating in a project focused on Generative Adversarial Networks (GANs). This work was not just an academic exercise—it became a cornerstone of my early career, leading to the publication of a research paper at the 2022 International Conference on Intelligent Technologies (CONIT). It was a defining moment that solidified my commitment to the field.

Determined to deepen my expertise, I pursued a master's degree in Business Analytics. Here, I expanded my knowledge of machine learning techniques, data visualization, and statistical analysis, applying these skills to large, complex datasets. One of the highlights of this period was my internship as an AI/ML Engineer at AI-Vets, where I took on the challenge of developing an object detection model using the YOLOv5 algorithm. This experience honed my abilities in model training, hyperparameter tuning, and performance evaluation, and it also sparked a new level of creativity in solving real-world problems.

Throughout this journey, my curiosity drove me to undertake extensive research and personal projects. I immersed myself in analyzing real-time and publicly available data, always searching for meaningful insights that could make a difference.

Today, I channel this passion and expertise into my role as a Data Analyst at the Illinois Department of Healthcare and Family Services. I'm part of the Integrated Eligibility System (IES) project, where I manage and analyze large-scale eligibility enrollment data across the state. Using SQL, DB2, Teradata, and Excel, I ensure that the data tells its story clearly and accurately. I also leverage SAP Business Objects - Web Intelligence to automate reports and support critical programs like SNAP, TANF, and Medicaid. In parallel, I also worked as a BI Developer at Advent Health, where I led the migration of credentialing data from MSOW to MD-Staff, designing and implementing ETL processes to integrate multiple data sources and reduce credentialing errors. I integrated MD-Staff data with Epic EHR using HL7 protocols and SQL transformations, streamlining reporting and eliminating 40+ hours of manual entry per month. I also automated data validation and cleaning in Python, saving analysts time and improving reporting accuracy. Additionally, I built and deployed interactive dashboards in Tableau and Power BI, providing actionable insights to senior leadership and contributing to higher patient satisfaction. I ensured data security and compliance with HIPAA and data governance standards and performed User Acceptance Testing (UAT) to validate system functionality and data integrity. Every day, I'm driven by the belief that with the right tools and mindset, data can be a powerful force for informed decision-making and positive change. </p>
								</header>
								<a class="image main"><img src="images/me3.jpg" alt="" /></a>
								
							</article>
							<div id="skills">

								<!-- Featured Post -->
									<article class="post featured">
										<header class="major">
								 <h2>Skills</h2>
								<table class="table-style">
								  <tr>
									<td>SQL</td>
									<td>⭐⭐⭐⭐⭐</td>
								  </tr>
								  <tr>
									<td>Python</td>
									<td>⭐⭐⭐⭐⭐</td>
								  </tr>
								  <tr>
									<td>MS Excel</td>
									<td>⭐⭐⭐⭐</td>
								  </tr>
								  <tr>
									<td>Power BI</td>
									<td>⭐⭐⭐⭐⭐</td>
								  </tr>
								  <tr>
									<td>Tableau</td>
									<td>⭐⭐⭐⭐⭐</td>
								  </tr>
								  <tr>
									<td>Machine Learning</td>
									<td>⭐⭐⭐⭐⭐</td>
								  </tr>
								  <tr>
									<td>JIRA</td>
									<td>⭐⭐⭐⭐</td>
								  </tr>
								  <tr>
									<td>Oracle</td>
									<td>⭐⭐⭐⭐</td>
								  </tr>
								  <tr>
									<td>Microsost Office Suite</td>
									<td>⭐⭐⭐⭐⭐</td>
								  </tr>
								  <tr>
									<td>R</td>
									<td>⭐⭐⭐</td>
								  </tr>
								<tr>
									<td>Six Sigma</td>
									<td>⭐⭐⭐⭐⭐</td>
								  </tr>
								</table>
							  </div>
					<style>
						/* Your existing styles here */
					
						/* Style the experience section */
						#experience {
						  margin-top: 2em;
						}
					
						#experience h2 {
						  font-size: 1.5em;
						  margin-bottom: 0.5em;
						}
					
						#experience .job {
						  margin-bottom: 1.5em;
						}
					
						#experience .job h3 {
						  font-size: 1.2em;
						  margin-bottom: 0.5em;
						}
					
						#experience .job p {
						  font-size: 1em;
						  margin-bottom: 0.5em;
						}
					  </style>
					</head>
					<body class="is-preload">
					
					<div id="experience">

						<!-- Featured Post -->
							<article class="post featured">
								<header class="major">
					
					 
							<h2>Experience</h2>
						  </header>

						  <div class="job">
							<h3>AI/ML Engineer Intern</h3>
							<p>Ameriinfo Vets</p>
							<p>Jan 2023 - Aug 2023</p>
							<p>I spearheaded the automation of PPE recognition in industrial settings by deploying YOLOv5 and integrating it with the DOTMLPF-P framework, aligning the technology with safety protocols and achieving a 25% reduction in safety violations. Additionally, I implemented a streamlined Data Conversion Lifecycle for PPE recognition, leveraging Linux shell scripts for efficient preprocessing of a 50,000-image dataset, contributing to a 25% reduction in safety violations and enhancing system documentation. My efforts in documenting the System Lifecycle through PyTorch model fine-tuning resulted in a 15% precision increase and an impressive 80% decrease in workplace accidents, demonstrating a comprehensive approach to safety and efficiency.</p>
						  </div>

						  	<div class="job">
							<h3>BI Deveoper</h3>
							<p>Advent Health</p>
							<p>February 2024 - December 2024</p>
							<p>In this role, I led the migration of credentialing data from MSOW to MD-Staff, designing and implementing ETL processes to integrate multiple data sources and reduce credentialing errors. I integrated MD-Staff data with Epic EHR using HL7 protocols and SQL transformations, streamlining reporting and eliminating 40+ hours of manual entry per month. I also automated data validation and cleaning in Python, saving analysts time and improving reporting accuracy. Additionally, I built and deployed interactive dashboards in Tableau and Power BI, providing actionable insights to senior leadership and contributing to higher patient satisfaction. I ensured data security and compliance with HIPAA and data governance standards and performed User Acceptance Testing (UAT) to validate system functionality and data integrity.</p>
						  </div>
					
					
						  <!-- Job 1 -->
						  <div class="job">
							<h4>Data Analyst</h4>
							<p>Illinois Department of Healthcare and Family Services</p>
							<p>February 2024 - Present</p>
							<p>I contribute to the Integrated Eligibility System (IES) project, where I manage and analyze extensive eligibility enrollment data for state programs such as SNAP, TANF, CHIP, and Medicaid. I leverage advanced SQL querying techniques to work with IBM Db2 and Teradata databases, focusing on optimizing code for performance and accuracy. My responsibilities include conducting Eligibility Determination (ED) by writing and executing Db2 code within the DBeaver environment. I also automate complex data processing workflows in SAP Business Objects and execute Extract, Transform, Load (ETL) processes using Informatica to ensure the efficient integration and transformation of large datasets. Additionally, I employ SAP Business Objects - Web Intelligence to generate and automate scheduled Performance Indicator (PI) reports, which are crucial for delivering timely insights to the Centers for Medicare & Medicaid Services (CMS) and supporting data-driven decision-making processes across the department.</p>
						  </div>
					
					
						  <!-- Repeat for other jobs -->
					
						</article>
					  </div>
			
				</div>
				<hr>
				<!-- Main -->
					

						<!-- Posts -->
						<article class="projects">
							<header class="major">
								
										<h1>Projects<br />
										</a></h1>
					
									
									<p>I've successfully navigated diverse industries, showcasing a broad spectrum of data analytics skills. From telecom and energy to finance and online marketplaces like Airbnb, I've gathered, cleaned, and analyzed extensive datasets. Utilizing tools such as Ms Excel, Python, SQL,Power BI and Tableau, I've demonstrated proficiency in data cleaning, statistical modeling, and feature engineering. My predictive modeling expertise shines through projects such as telecom customer churn analysis, US power generation insights, personal loan acceptance prediction, loan default and loss severity assessment, image classification using ConvNets, sentiment analysis on IMDb reviews, and crafting targeted insights dashboards in Power BI and the addition of navigating Airbnb prosperity exemplifies my ability to extract valuable insights from historical data using Tableau, further emphasizing my diverse and robust skill set as a data analyst.</p>
									<ul class="actions special">
										
									</ul>
									</header>
						<article>
							<header>
								
										<h3>Customer Churn at a Telecom Company<br />
										</a></h3>
					                </header>
									<a class="image fit"><img src="images/churn.jpg",width="1" 
										height="300"  alt="" /></a>
									<p>This project seamlessly integrated various tools and techniques to tackle the complex issue of predicting customer churn in the telecom industry. Utilizing SQL for data manipulation and Excel for efficient data storage, we ensured a structured and accessible dataset. Python played a pivotal role in the exploratory data analysis (EDA) phase, allowing us to uncover patterns and trends within the data. Additionally, Decision Tree was employed for predictive modeling, creating a robust system capable of accurately forecasting customer churn. This multi-faceted approach, combining SQL, Excel, and Python, not only facilitated a comprehensive understanding of the dataset but also enabled the development of a powerful predictive model, contributing to successful outcomes in mitigating customer churn.</p>
									<ul class="actions special">
										<li><a href="https://github.com/Niharika1307/Churn-Prediction/blob/main/churn/Churn_Decision_Tree.ipynb" class="button">View Project</a></li>
									</ul>
									
								</article>
								
								<article>
									<header>
							
										<h3>US Power Generation: K-Means Insights from PUDL Dataset<br />
										</a></h3>
									</header>
									<a class="image fit"><img src="images/FUEL.jpg" alt="" /></a>
									<p>This analysis delves into the Public Utility Data Preparation (PUDL) dataset, employing the K-Means clustering algorithm to efficiently segment vast data for insights into US power generation. By determining optimal clustering (k=3) through the WSS and Silhouette methods, the study identifies three distinct clusters. Notably, while Cluster 1, dominated by coal, is cost-efficient, it carries environmental concerns. In contrast, Cluster 2, featuring natural gas, emerges as a promising choice with reasonable costs and minimal impurities. The findings advocate for Natural Gas as an environmentally friendly and cost-effective fuel for US power generation. The K-Means approach proves invaluable for its speed, making it a practical choice for clustering extensive datasets.</p>
									<ul class="actions special">
										<li><a href="https://github.com/Niharika1307/64060_-nkollibo/blob/main/FINAL%20PROJECT/Fuel_cost%20(1).ipynb" class="button">View Project</a></li>
									</ul>
								</article>
								<article>
									<header>
										
										<h3>Predicting Personal Loan Acceptance: Naive Bayes Analysis on Universal Bank Data<br />
										</a></h3>
									</header>
									<a class="image fit"><img src="images/LOAN.jpg" alt="" /></a>
									<p>Conducting a comprehensive analysis on 5000 Universal Bank customers, this study utilized Python, along with a diverse set of tools including EDA, SQL, Excel, and statistical analysis. Focusing on predictors like Online and Credit Card, and employing Naive Bayes, the analysis successfully predicted personal loan acceptance. Out of the sampled customers, 9.6% accepted the offered personal loan. The model, enriched by demographic information and the customer's bank relationship, achieved an impressive accuracy of 90.35%. This holistic approach involving diverse tools showcases the multifaceted nature of the analysis, ensuring robust insights for targeted marketing strategies and informed decision-making in personal loan campaigns.</p>
									<ul class="actions special">
										<li><a href="https://github.com/Niharika1307/64060_-nkollibo/blob/main/Assignment_3/Naive_Bayes.ipynb" class="button">View Project</a></li>
									</ul>
								</article>
								<article>
									<header>
										
										<h3>Predicting Loan Default and Loss Severity using Random Forest Classifier<br />
										</a></h3>
									</header>
									<a class="image fit"><img src="images/Loss.jpg" alt="" /></a>
									<p>This project challenges the conventional approach to loan default prediction by not only distinguishing between good and bad counterparties but also anticipating and incorporating the severity of potential losses. Shifting from the binary classification of traditional banking, this endeavor bridges the gap between minimizing economic capital consumption and optimizing risk for financial investors. The evaluation metric for this competition is the mean absolute error (MAE). The project encompasses data exploration analysis, utilizing a modeling strategy grounded in Logistic Regression, and further enhancing the predictive capability with the application of a Random Forest Classifier. The estimation of model performance, guided by the MAE metric, provides a holistic perspective on anticipating loan defaults and assessing the associated loss severity, offering valuable insights for risk optimization in asset management.</p>
									<ul class="actions special">
										<li><a href="https://github.com/Niharika1307/BankLoanLossSeverity/blob/main/Project/ADM_FINAL%20(2).ipynb" class="button">View Project</a></li>
									</ul>
								</article>
								<article>
									<header>
										
										<h3>Enhancing Image Classification: Convnets for Cats & Dogs Using Deep Learning Techniques<br />
										</a></h3>
									</header>
									<a class="image fit"><img src="images/catsdog" alt="" /></a>
									<p>The project focuses on leveraging deep learning for computer vision tasks, specifically classifying images in the "Dog-vs-Cats" subset. Despite the challenge posed by limited data, a convolutional neural network (convnet) is developed from scratch, emphasizing its ability to learn spatial patterns in images. The dataset is meticulously prepared, comprising subsets for training, validation, and testing. Techniques such as preprocessing, data augmentation, and incorporating a pre-trained model (VGG16) are employed to enhance model performance. The results indicate that pre-trained models consistently outperform those trained from scratch, especially when dealing with restricted training data. Notably, increasing the training set size and adjusting validation set sizes contribute to improved model accuracy. Overall, the project underscores the effectiveness of deep learning techniques in image classification, even with limited datasets, offering insights into optimal model architectures and training strategies.
									</p>
									<ul class="actions special">
										<li><a href="https://github.com/Niharika1307/64061_-nkollibo/blob/main/Assignment-2/Cats_vs_Dogs.ipynb" class="button">View Project</a></li>
									</ul>
								</article>
								<article>
									<header>
										
										<h3>Sentiment Analysis on IMDB Reviews: Custom vs. Pre-trained Embedding Layers<br />
										</a></h3>
									</header>
									<a class="image fit"><img src="images/imdb.jpg",width="200",heights="100" alt="" /></a>
									<p>In this sentiment analysis study on IMDB movie reviews, we explored two embedding techniques: a custom-trained embedding layer and a pretrained word embedding layer using GloVe. Our dataset comprised 50,000 reviews, truncated after 150 words, with training samples ranging from 100 to 10,000. The custom-trained embedding layer showed high accuracy (97.5% to 98.5%) with optimal performance at a training sample size of 1000. Meanwhile, the pretrained GloVe model achieved accuracy ranging from 92.9% to 100%, peaking at a sample size of 100 but showing signs of overfitting with larger samples. Overall, the custom-trained embedding layer demonstrated superior performance, especially with larger training sets.
									</p>
									<ul class="actions special">
										<li><a href="https://github.com/Niharika1307/64061_-nkollibo/blob/main/Assignment-3/IMDB_EMBEDDING%20(1).ipynb" class="button">View Project</a></li>
									</ul>
								</article>
								<article>
									<header>
										
										<h3>Empowering the Portage APL with Targeted Insights:Dashboard in Power BI<br />
										</a></h3>
									</header>
									<a class="image fit"><img src="images/APL.jpg" alt=""/></a>
									<p>In a proactive initiative, delved into a real-time dataset to enhance strategic decision-making.Leveraging Tableau and Power BI,crafted a predictive dashboard,utilizing Data Analysis Expressions (DAX) measures and SQL for data manipulation.Excel played a pivotal role in Exploratory Data Analysis (EDA).
										The dashboard provides a holistic view, guiding resource allocation and shedding light on priority animal categories.The project explored correlations such as age impacting adoptions/euthanizations, city-wise adoption and euthanization trends,and year-wise analyses.Recommendations encompassed implementing background checks, event data collection, tracking expenses per animal, and documenting donation dates. This multifaceted approach equips Portage APL with actionable insights for the welfare of animals, employing advanced analytics techniques.
									</p>
						
									<ul class="actions special">
										<li><a href="https://app.powerbi.com/groups/me/reports/65460920-7b17-4ab3-9b79-e2c3a7ffcabd/ReportSection?experience=power-bi" class="button">View Project</a></li>
									</ul>
								</article>
								<article>
									<header>
										
										<h3>Navigating Airbnb Prosperity: Insights from Historical Data Unveiled with Tableau<br />
										</a></h3>
									</header>
									<a class="image fit"><img src="images/Dashboard 1.png" alt="" /></a>
									<p>Empowering aspiring entrepreneurs in the Airbnb realm, this meticulously crafted Tableau dashboard, driven by SQL-refined datasets, enables strategic decision-making. Prospective hosts gain a comprehensive view of demand dynamics, allowing precise decisions on optimal city locations, preferred property types, and specific room configurations. Moreover, the tool delves into seasonal patterns, ensuring hosts can strategically lease their properties to meet fluctuating demand throughout the year. This comprehensive approach provides entrepreneurs with actionable insights to make informed choices, maximizing their success in the Airbnb business.
									</p>
									<ul class="actions special">
										<li><a href="https://public.tableau.com/app/profile/niharika.kolliboyana2259/viz/shared/QNDWYTR47" class="button">View Project</a></li>
									</ul>
								</article>
								
					</article>
					<hr>
								<!-- Posts -->
						<article class="research">
							<header class="major">
								
										<h2>Research<br />
										</a></h2>
					
									
									<p>My research endeavors reflect a profound motivation to address contemporary challenges. The publication on "Improved Bot Identification with Imbalanced Data Using GG-XGBoost" demonstrates my commitment to countering the impact of bots on online social networks. The project on "Rail-Road Worker Safety Enhancement Through Computer Vision-Based Detection of Safety Helmets and Vests Using YOLOv5" reflects my passion for leveraging advanced computer vision techniques to revolutionize safety in the railroad industry. Grounded in diverse datasets and meticulous evaluations, these endeavors showcase my dedication to providing meaningful solutions in the realms of cybersecurity and occupational safety.</p>
									<ul class="actions special">
										
									</ul>
									</header>
						
							<article>
								<header>
									
									<h3>An Improved Bot Identification with Imbalanced Data using GG-XGBoost<br />
									</a></h3>
								</header>
								<a class="image fit"><img src="images/IEEE.1.jpg" alt="" /></a>
								<p>This work has been published in the proceedings of the 2022 2nd International Conference on Intelligent Technologies (CONIT).In the realm of online social networks (OSNs), the proliferation of bots disseminating fraudulent information poses a significant challenge. Despite being outnumbered by benign users, bots have a disproportionately negative impact. Conventional bot identification methods, reliant on supervised learning, often grapple with imbalanced datasets featuring fewer bot instances. This work introduces a novel approach, leveraging a generative adversarial network (GAN) with gated recurrent unit (GRU), to address the unbalanced bot distribution in OSNs. Additionally, a pioneering algorithm, GG-XGBoost, is proposed, seamlessly integrating GRU-GAN with the XGBoost model. Experimental validation on Twitter datasets underscores the efficacy of the GG-XGBoost algorithm in enhancing bot identification accuracy.
								</p>
								<ul class="actions special">
									<li><a href="https://github.com/Niharika1307/DMSB-VAE-GAN-" class="button">View Research</a></li>
								</ul>
							</article>
						
								<article>
									<header>
										
										<h3>Rail-Road Worker Safety Enhancement through 
											Computer Vision-Based Detection of Safety Helmets and 
											Vests using YOLOv5 <br />
										</a></h3>
									</header>
									<a class="image fit"><img src="images/rail.jpg" alt="" /></a>
									<p>This research focuses on enhancing rail-road worker safety through computer vision-based detection of safety helmets and vests using YOLOv5. The study addresses the challenge of monitoring personal protective equipment (PPE) compliance, crucial for minimizing workplace accidents. Traditional methods for PPE monitoring are manual, time-consuming, and prone to errors. The proposed solution involves a deep learning approach using YOLOv5, which proves effective in detecting safety helmets and vests worn by rail-road workers.

										The dataset used for training comprises images from various industrial settings, ensuring a diverse and representative set. The YOLOv5 architecture, pretrained on the COCO dataset, is employed for object detection. The model is fine-tuned and evaluated on precision and recall metrics. The results show high precision and recall values for safety helmets and vests, indicating the model's efficacy in identifying PPE.
										
										The study emphasizes the potential of YOLOv5 as a powerful tool for improving safety in the railroad industry. The automated PPE compliance monitoring system can contribute to a safer working environment by ensuring workers wear the required safety gear. The research also acknowledges challenges such as GDPR compliance and hardware requirements. Overall, the proposed approach presents a promising solution to enhance rail-road worker safety through advanced computer vision techniques.
									</p>
									<ul class="actions special">
										<li><a href="https://github.com/Niharika1307/64061_-nkollibo/blob/main/PAPER/Final_IEEE.pdf" class="button">View Research</a></li>
									</ul>
								</article>
					</article>
					<hr>
					<article class="case study">
						<header class="major">
							
									<h2>Case Study<br />
									</a></h2>
				
								
								<p>Exploring six diverse analytics case studies reveals crucial lessons, emphasizing the importance of meticulous planning, transparent communication, and continuous improvement across various industries.</p>
								<ul class="actions special">
									
								</ul>
								</header>
					
						<article>
							<header>
								
								<h3>Insights and Imperfections: A Deep Dive into Six Diverse Analytics Case Studies<br />
								</a></h2>
							</header>
							<a href="#" class="image fit"><img src="images/cs.jpg" alt="" /></a>
							<p>In examining six distinct case studies, various challenges and lessons in analytics projects emerge. The Boeing 737 MAX incidents underscore the critical need for robust risk management, transparent communication, and vigilant testing in safety-critical domains. The banking fraud detection case emphasizes the significance of considering performance requirements and conducting real-world testing. Amazon Rekognition's misidentification issues highlight the importance of diverse training data and transparency. IBM Watson's healthcare misstep emphasizes the necessity of thorough planning, unbiased data, and collaboration. The failure of AI for university admission suggests the importance of domain expertise and quality data. The loss of the Mars Orbiter stresses the need for effective communication, standardized measurement units, and adequate resource allocation. Collectively, these cases underscore the imperative for meticulous planning, diverse testing, transparent communication, and continuous improvement in analytics projects across various industries.
							</p>
							<ul class="actions special">
								<li><a href="https://github.com/Niharika1307/AIP-/tree/main/Analytical%20Case%20Study" class="button">View Case Study</a></li>
							</ul>
						</article>
							</section>
							<hr>
							<div id="main">

								<!-- Featured Post -->
									<article class="education">
										<header class="major">
							<h1>Education</h1>
							  <table class="table-style">
								<tr>
								  <td>Master's in Business Analytics at Kent State University</td>
								  <td>2022-23</td>
								</tr>
								<tr>
								  <td>Bachelor's in Computer Science & Engineering at GVPCEW</td>
								  <td>2018-22</td>
								</tr>
							  </table>
							</div>
				
							<hr>
							<div id="main">

								<!-- Featured Post -->
									<article class="certification">
										<header class="major">
							<h1>Certifications</h1>
							  <table class="table-style">
								<tr>
								  <td>Google Data Analytics</td>
								  <td>Coursera</td>
								</tr>
								<tr>
									<td>
										HackerRank SQL Assessment</td>
									<td>HackerRank</td>
								  </tr>
								  
								  <tr>
									<td>Agile Scrum Certifications</td>
									<td>SCRUMstudy</td>
								  </tr>
								<tr>
								  <td>
									Six Sigma Yellow Belt</td>
								  <td>VMEdu Inc.</td>
								</tr>
								<tr>
									<td>Design a data ingestion strategy for ML projects</td>
									<td>Microsoft</td>
								  </tr>
									<tr>
									<td>Google AI Explore ML Beginner & Advanced</td>
									<td>Google</td>
								  </tr>
								<tr>
									<td>Python for Machine Learning & Data Science Masterclass</td>
									<td>Udemy</td>
								  </tr>
								
							  </table>
							</div>
                        
						<!-- Footer -->
							

					</div>

				<!-- Footer -->
					<footer id="footer">
						
						<section class="split contact">
							<section class="alt">
								<h3>Address</h3>
								<p>San Jose,CA 95134<br />
								</p>
							</section>
							
							<section>
								<h3>Email</h3>
								<p><a href="#">kolliboyananiharika@gmail.com</a></p>
							</section>
							
							<section>
								<h3>Social</h3>
								<ul class="icons alt">
									<li><a href="https://www.linkedin.com/in/niharikabk/" class="icon brands fa-linkedin"><span class="label">linkedin</span></a></li>
				                    <li><a href="https://github.com/Niharika1307" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
									<li><a href="https://newopinions8.wordpress.com/" class="icon brands fa-wordpress"><span class="label">Blog</span></a></li>
								</ul>
							</section>
						</section>
					</footer>

				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; Copyright</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li></ul>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>